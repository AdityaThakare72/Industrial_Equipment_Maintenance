** DVC

since we are using dvc not going to put data folder in the gitignore file as once we add data (tracking through dvc) it automatically creates a file (with .dvc format) which tells git to ignore it

so just add the data file to dvc with => dvc add <file path>

NOTE: mnake sure we add the data file to dvc first and then git commit otherwise if we do viceversa then git will track the data file as well. If that happens => git rm --cached data/raw/industrial_equipment_anomaly_data.csv
dvc add data/raw/industrial_equipment_anomaly_data.csv


** 
Now before moving lets keep the main branch separate

COMMANDS FOR BRANCHING =>

1.
# This creates a branch and switches you to it immediately
git checkout -b feat/data-ingestion

The -b flag means Create a new branch.

2.
Now you write your dataset.py code. You commit your changes as you go.
git add industrial_maintenance/dataset.py
git commit -m "feat: implement raw data loading and initial cleaning"
These commits only exist in the feat/data-ingestion

3.
If you need to see your "stable" code again:
git checkout main
Suddenly, your dataset.py goes back to being empty! Don't worry, your work is safe in the other branch.

4.
Merging
Once you are 100% sure your code works and is clean, you bring that "future" into the "present":
# Switch to main first
git checkout main

# Pull the changes from the feature branch into main
git merge feat/data-ingestion


NOTE: the content of the new branch you create will depend on => on which branch u were when u created this new one


###### Essential Commands ######

Command	                                        Purpose

git branch	                          Lists all your current "Parallel Universes."
git checkout -b <name>	              Create a new universe and jump into it.
git checkout <name>	                  Jump between existing universes.
git merge <branch name>               when on main merge other branch with it (switch to main first before doing)
git branch -d <name>	              Destroy a universe (after you've merged it).
git push origin <name>	              Send your specific branch to GitHub so others can see it. 



########################### DVC PIPELINE ####################################

If we start using DVC Pipelines (dvc.yaml) now, our future GitHub Actions (the CI/CD ) will be able to run our entire experiment automatically whenever you push new code or data.

The "Automation" (dvc.yaml)

Instead of you manually running python dataset.py and then dvc add, we will define a Stage. This stage tells DVC the three things:

    Dependencies (deps): What does this script need to run? (The code and the raw data).

    Command (cmd): How do we run it?

    Outputs (outs): What does it produce? (The interim data).




Step 1: Create the dvc.yaml File

In your project root (on your exp-branch), create a file named dvc.yaml

stages:
  ingest:
    cmd: python industrial_maintenance/dataset.py
    deps:
      - industrial_maintenance/dataset.py
      - data/raw/industrial_equipment_anomaly_data.csv
    outs:
      - data/interim/cleaned_data.csv


Step 2: The "Reproduction" Ritual

Now, delete your data/interim/cleaned_data.csv manually. Then, instead of running python, run this command in your Arch terminal:
dvc repro

What happens? DVC looks at the "Scroll" (dvc.yaml), sees that the output is missing, checks that the dependencies are there, and runs the command for you. It then automatically handles the tracking of data/interim/cleaned_data.csv.



###################################

dataset.py => ingest
features.py => feature engg

####################

for training... -> modeling/train.py
for hyper-parameters -> params.yaml  (also added the model as parameter)




############## CLASS IMBALANCE (Algorithm Level Solution)  ###############
class_weight='balanced' and scale_pos_weight—is an Algorithm-Level solution. Instead of changing the data, we change the Loss Function (the model's "Cost of Failure").

The Core Difference

Think of it this way:

    SMOTE/Oversampling: You invite more "rare" students to the classroom so the teacher sees them more often.

    Class Weighting (The Algorithm Way): You don't invite more students, but you tell the teacher: "Every time you fail to help a 'rare' student, it counts as 10 mistakes on your evaluation, but failing a common student only counts as 1".

    Why is the "Algorithm Way" often better?

    No Ghost Data: SMOTE can create "noisy" data points that don't actually exist in the real world.

    Speed: You don't increase your dataset size, so your training stays fast—essential for your Arch Linux workflow.

    Memory: You don't need to store millions of duplicate or synthetic rows.

    1. For Random Forest: class_weight='balanced'
    When you set this, Scikit-learn automatically calculates a weight (wj​) for each class (j) using this formula:

            wj​=nclasses​×nsamples,j​nsamples​​

    nsamples​: Total samples.

    nclasses​: Number of classes (for us, 2).

    nsamples,j​: Number of samples in that specific class.

Result: If failures are rare, they get a massive weight. During training, the Random Forest will work much harder to correctly split the nodes for those rare failures.


2. For XGBoost: scale_pos_weight

XGBoost uses a single number to scale the "Gradient" (the error signal) for the positive class (the failures).

The standard formula we used is:
scale_pos_weight=count(PositiveSamples)count(NegativeSamples)​

Scenario:

    95 Healthy machines (Negative).

    5 Faulty machines (Positive).

    scale_pos_weight = 95 / 5 = 19.

Now, every time XGBoost trains, the error from a "Faulty" machine is treated as 19 times more important than the error from a "Healthy" one.



################ Fats API ##########################

app/main.py  file

to actually check

In your browser, change the URL from http://127.0.0.1:8000/ to:

    http://127.0.0.1:8000/docs


################## Done with streamlit  ################

app/dashboard file

##################### To run the streamlit file  ################

need to run following in the order in different terminals

conda activate industrial_maintenance
mlflow ui
uvicorn app.main:app --reload
streamlit run app/dashboard.py

#### Connection Summary ####

Service    Port    Talk To                  Protocol

MLflow     5000    Filesystem (mlruns)      HTTP (Internal)
FastAPI    8000    MLflow (Port 5000)       REST API
Streamlit  8501    FastAPI (Port 8000)      REST API




########################  DEPLOYMENT #####################
##########################################################

---------> digital ocean <--------------

------> 1 <-------
On your local Arch machine, create a bundle of the code and the model's memory.

# Tar up everything needed (excluding local env folders)
tar -cvzf maintenance_native.tar.gz app/ mlruns/ mlflow.db requirements.txt